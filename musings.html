<!DOCTYPE HTML>


<html>
	<head>
		<title>Scott Seidel</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="icon" href="media/icon1.png" type="image/png">
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo">Scott Seidel</a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li><a href="index.html">About</a></li>
							<li><a href="experience.html">Experience</a></li>
							<li class="active"><a href="musings.html">Musings</a></li>
						</ul>
						
						<ul class="icons">
							<li><a href="https://www.linkedin.com/in/scott-seidel" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
<!--
							<li><a href="#" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
							<li><a href="#" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
							<li><a href="#" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li>
							<li><a href="#" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
-->
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Posts -->
 
							<section class="posts">

								<article>
									<header>
										<span class="date">August 6, 2024</span>
										<h2><a href="posts/llm006.html">LLM Collapse from Self-Generated Data</a></h2>
									</header>

									<a href="posts/llm006.html" class="image fit"><img src="media/gai_cycle.png" alt="" /></a>

									<p>The post examines the theoretical risk of model collapse in AI, focusing on a study by Shumailov et al. that explores a worst-case scenario where excessive synthetic data degrades model performance. While this issue is not currently prevalent in LLMs, the study provides insights into how over-reliance on synthetic data could potentially lead to reduced data diversity and accuracy in the future.</p>
									
									<ul class="actions special">
										<li><a href="posts/llm005.html" class="button">Full Post</a></li>
									</ul>								
								</article>

								<article>
									<header>
										<span class="date">July 30, 2024</span>
										<h2><a href="posts/llm005.html">Energy of LLMs</a></h2>
									</header>

									<a href="posts/llm005.html" class="image fit"><img src="media/training_flops.png" alt="" /></a>

									<p>This post explores the substantial compute and energy requirements of LLMs, focusing on Chat GPT-3. It details how increasing model parameters significantly raises the demand for computational power, exemplified by the usage of petaflop/s-days. The post also compares these requirements to the capabilities and energy consumption of the world's fastest supercomputer, Frontier, to highlight the scale of resources involved.</p>
									
									<ul class="actions special">
										<li><a href="posts/llm005.html" class="button">Full Post</a></li>
									</ul>								
								</article>


								<article>
									<header>
										<span class="date">July 23, 2024</span>
										<h2><a href="posts/llm004.html">Role of Data Quality in Training LLMs</a></h2>
									</header>

									<a href="posts/llm004.html" class="image fit"><img src="media/gpt3_data.png" alt="" /></a>

									<p>Data quality plays a crucial role in training large language models like GPT-3. This post delves into the decisions made by OpenAI to optimize data for GPT-3's training, from filtering vast datasets to incorporating high-quality sources. It highlights how a combination of Wikipedia, WebText2, Books1, and other curated data was essential for achieving superior model performance. Learn about the importance of data quality and the strategies used to enhance LLM training.</p>
									
									<ul class="actions special">
										<li><a href="posts/llm004.html" class="button">Full Post</a></li>
									</ul>								
								</article>


								<article>
									<header>
										<span class="date">July 16, 2024</span>
										<h2><a href="posts/llm003.html">Expansive Landscape of LLM Training Data</a></h2>
									</header>

									<div class="video-container">
							            <video autoplay muted loop class="video">
							                <source src="media/code_scroll1.mp4" type="video/mp4">
							                Your browser does not support the video tag.
							            </video>
							    	</div>
									<p><br />This post examines the extensive data sources used in training large language models (LLMs), such as web pages, digitized books, and scholarly articles. It provides an in-depth analysis of the estimated amounts of text data available and discusses how models like GPT-3 utilize only a small fraction of this data. The focus is on understanding how much additional data is available as models continue to exponentially grow in size.</p>
									
									<ul class="actions special">
										<li><a href="posts/llm003.html" class="button">Full Post</a></li>
									</ul>								
								</article>


								<article>
									<header>
										<span class="date">July 9, 2024</span>
										<h2><a href="posts/tpm001.html">Benefits of Centralized TPM Teams</a></h2>
									</header>

									<div class="video-container">
							            <video autoplay muted loop class="video">
							                <source src="media/collab1.mp4" type="video/mp4">
							                Your browser does not support the video tag.
							            </video>
							    	</div>
									<p><br />TPM teams can be a great value add for any organization that has technical cross-functional programs. However, they can also become bloated and ineffective if not structured correctly. Here we look at the need for TPM teams to be centralized.</p>
									
									<ul class="actions special">
										<li><a href="posts/tpm001.html" class="button">Full Post</a></li>
									</ul>								
								</article>


								<article>
									<header>
										<span class="date">July 5, 2024</span>
										<h2><a href="posts/llm002.html">LLM Model Comparison for Factual Recall</a></h2>
									</header>

									<a href="posts/llm002.html" class="image fit"><img src="media/llm_error_comparison_full1.png" alt="" /></a>

									<p>This is a second post on the recent study by Moaryeri et al. on bias in LLMs when recalling factual country statistics. The article also breaks down the individual performance of 20 different models used. It provides an interesting baseline in which to examine model design and performance.</p>
									
									<ul class="actions special">
										<li><a href="posts/llm002.html" class="button">Full Post</a></li>
									</ul>								
								</article>


								<article>
									<header>
										<span class="date">July 2, 2024</span>
										<h2><a href="posts/llm001.html">Geographic Bias in LLMs</a></h2>
									</header>

									<a href="posts/llm001.html" class="image fit"><img src="media/mean_region_error1.png" alt="" /></a>

									<p>A recent study by Moaryeri et al. examines the geographic bias in LLMs when recalling factual country statistics. The research highlights significant disparities in data accuracy across different regions and income levels, emphasizing the need to address these biases for more equitable AI development. This initial look into the persistent North American and European bias offers valuable insights into the challenges of creating globally fair algorithms.</p>
									
									<ul class="actions special">
										<li><a href="posts/llm001.html" class="button">Full Post</a></li>
									</ul>								
								</article>

								<article>
									<header>
										<span class="date">Month Day, Year</span>
										<h2><a href="#">More Coming Soon</a></h2>
									</header>								

								<div class="video-container">
							            <video autoplay muted loop class="video">
							                <source src="media/cs1.mp4" type="video/mp4">
							                Your browser does not support the video tag.
							            </video>
							    </div>
							    	<p><br />I will be adding future thoughts and porting over more of my previous thoughts on AI development, data collection studies, data diversity needs, and program management in this space.</p>
								</article>
							</section>



						<!-- Footer -->

<!--
							<footer>
								<div class="pagination">
-->
									<!--<a href="#" class="previous">Prev</a>-->
<!--
									<a href="#" class="page active">1</a>
									<a href="#" class="page">2</a>
									<a href="#" class="page">3</a>
									<span class="extra">&hellip;</span>
									<a href="#" class="page">8</a>
									<a href="#" class="page">9</a>
									<a href="#" class="page">10</a>
									<a href="#" class="next">Next</a>
								</div>
							</footer>
-->
					</div>


				<!-- Copyright -->
					<div id="copyright">
						<ul><li>&copy; 2024</li><li>Based in California</li></ul>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>